{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries to use\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras as kr\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data=np.genfromtxt('iris.csv',delimiter=',',dtype='str',header=None)\n",
    "data=list(csv.reader(open('iris.csv')))[1:]\n",
    "#list(data)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'setosa',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'versicolor',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica',\n",
       " 'virginica']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create inputs array with details and convert it from str to float\n",
    "inputs  = np.array(data)[:,:4].astype(np.float)\n",
    "# Create outputs array with species \n",
    "outputs = np.array(data)[:,4]\n",
    "#list(inputs)\n",
    "list(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# return_inverse——https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.unique.html\n",
    "# return the indices of the unique array ——each species gets a unique index in different position to represent their value \n",
    "outputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)\n",
    "\n",
    "# import this package to change type of index for next step that can match outputs data format \n",
    "from keras.utils import np_utils\n",
    "outputs_cats = kr.utils.to_categorical(outputs_ints)\n",
    "# test np.unique to return inverse value\n",
    "print(outputs_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "# Split the input and output data sets into training and test subsets.\n",
    "# reference to spilt data keras-iris——https://github.com/emerging-technologies/keras-iris/blob/master/iris_nn.py\n",
    "# np.random.permutation——https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.permutation.html\n",
    "# randomly  shuffle the elements of array\n",
    "\n",
    "\n",
    "inds = np.random.permutation(len(inputs))\n",
    "\n",
    "# make multiple sub-arrays to split data 75 by 75\n",
    "train_inds, test_inds = np.array_split(inds, 2)\n",
    "\n",
    "# match array of value to input and output type\n",
    "inputs_train, outputs_train = inputs[train_inds], outputs_cats[train_inds]\n",
    "inputs_test,  outputs_test  = inputs[test_inds],  outputs_cats[test_inds]\n",
    "# test outputs data\n",
    "print(outputs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders and variables. input has 4 attributes and output has 3 classes for species \n",
    "# x for inputs details with length of flowers and it can be fed data as same format\n",
    "x=tf.placeholder(tf.float32,shape=[None,4])\n",
    "# y for outputs details with species and it can be fed data as same format\n",
    "y=tf.placeholder(tf.float32,shape=[None,3])\n",
    "# Weight and bias\n",
    "W=tf.Variable(tf.zeros([4,3]))\n",
    "b=tf.Variable(tf.zeros([3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model of softmax to predict results\n",
    "prediction = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_entropy function to calculate the cross entropy\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimiser to train\n",
    "train_step = tf.train.AdamOptimizer(0.2).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialising variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "# Number of interations\n",
    "    for epoch in range(50):\n",
    "        c=sess.run([train_step], feed_dict={x: inputs_train, y:outputs_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating accuracy of our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 0,Testing Result 0.28,Testing Error Rate 0.719999998808\n",
      "Result 5,Testing Result 0.64,Testing Error Rate 0.360000014305\n",
      "Result 10,Testing Result 0.64,Testing Error Rate 0.360000014305\n",
      "Result 15,Testing Result 0.64,Testing Error Rate 0.360000014305\n",
      "Result 20,Testing Result 0.64,Testing Error Rate 0.360000014305\n",
      "Result 25,Testing Result 0.733333,Testing Error Rate 0.266666650772\n",
      "Result 30,Testing Result 0.96,Testing Error Rate 0.0400000214577\n",
      "Result 35,Testing Result 0.773333,Testing Error Rate 0.226666688919\n",
      "Result 40,Testing Result 0.8,Testing Error Rate 0.199999988079\n",
      "Result 45,Testing Result 0.96,Testing Error Rate 0.0400000214577\n",
      "Result 50,Testing Result 0.96,Testing Error Rate 0.0400000214577\n",
      "Result 55,Testing Result 0.946667,Testing Error Rate 0.0533333420753\n",
      "Result 60,Testing Result 0.946667,Testing Error Rate 0.0533333420753\n",
      "Result 65,Testing Result 0.973333,Testing Error Rate 0.0266666412354\n",
      "Result 70,Testing Result 0.96,Testing Error Rate 0.0400000214577\n"
     ]
    }
   ],
   "source": [
    "# tf.equal to decide what value should be returned (True is 1,False is 0) \n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "# accuracy gets a average result of correct_prediction\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# Test\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # times for training all training set\n",
    "    for epoch in range(75):    \n",
    "        #train  \n",
    "        sess.run(train_step,{x:inputs_train, y:outputs_train})                  \n",
    "        #test\n",
    "        acc = sess.run(accuracy,{x:inputs_test, y:outputs_test})     \n",
    "        #print test value every 5 times begin from the first time\n",
    "        if(epoch%5==0):\n",
    "            print(\"Result \" + str(epoch) + \",Testing Result \" + str(acc) + \",Testing Error Rate \" + str(1-acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Accurency generates different results and accuracy rate keep at 96%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
